{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'[', u'Moby', u'Dick', u'by', u'Herman', u'Melville', u'1851', u']', u'ETYMOLOGY', u'.']\n"
     ]
    }
   ],
   "source": [
    "# inspect the start of the first book\n",
    "print [t for t in text1[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u',', 18713),\n",
       " (u'the', 13721),\n",
       " (u'.', 6862),\n",
       " (u'of', 6536),\n",
       " (u'and', 6024),\n",
       " (u'a', 4569),\n",
       " (u'to', 4542),\n",
       " (u';', 4072),\n",
       " (u'in', 3916),\n",
       " (u'that', 2982)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the words and display the most frequent\n",
    "from collections import Counter\n",
    "text1_counts = Counter()\n",
    "text1_counts.update(text1)\n",
    "text1_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the number of unique words\n",
    "len(text1_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u',', 9397),\n",
       " (u'to', 4063),\n",
       " (u'.', 3975),\n",
       " (u'the', 3861),\n",
       " (u'of', 3565),\n",
       " (u'and', 3350),\n",
       " (u'her', 2436),\n",
       " (u'a', 2043),\n",
       " (u'I', 2004),\n",
       " (u'in', 1904)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same for Sense and Sensibility\n",
    "text2_counts = Counter()\n",
    "text2_counts.update(text2)\n",
    "print len(text2_counts)\n",
    "text2_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4669\n"
     ]
    }
   ],
   "source": [
    "# determine the shared vocabulary\n",
    "shared_vocab = list(set(text1_counts.keys()).intersection(text2_counts.keys()))\n",
    "shared_vocab.sort()\n",
    "print len(shared_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4669)\n"
     ]
    }
   ],
   "source": [
    "# extract the counts of the shared vocabulary and put it into a matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "text1_vector = [float(text1_counts[t]) for t in shared_vocab]\n",
    "text2_vector = [float(text2_counts[t]) for t in shared_vocab]\n",
    "feature_matrix = np.vstack([text1_vector, text2_vector])\n",
    "# normalize the counts to get frequencies\n",
    "feature_matrix = normalize(feature_matrix, axis=1)\n",
    "print feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=5e-05, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# create a sparse regression model\n",
    "model = Lasso(alpha=0.00005)\n",
    "\n",
    "# assign labels \n",
    "y = [1,0]\n",
    "\n",
    "# fit the model\n",
    "# NOTE: this is only an illustrative example, not the proper way to do things.\n",
    "# This model will not converge because we only have two examples!\n",
    "model.fit(X=feature_matrix, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\ttext1\ttext2\tweight\n",
      "her\t329\t2436\t-4.137692\n",
      "the\t13721\t3861\t1.706455\n"
     ]
    }
   ],
   "source": [
    "# display the words that have non-zero weights\n",
    "indices = [i for i, v in enumerate(shared_vocab) if model.coef_[i] != 0]\n",
    "print \"word\\ttext1\\ttext2\\tweight\"\n",
    "for i in indices:\n",
    "    word = shared_vocab[i]\n",
    "    print \"%s\\t%d\\t%d\\t%f\" % (word, text1_counts.get(word), text2_counts.get(word), model.coef_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.98542581]\n",
      "her: 51\n",
      "the: 4045\n"
     ]
    }
   ],
   "source": [
    "# apply the classifier to the WSJ\n",
    "test_counts = Counter()\n",
    "test_counts.update(text7)\n",
    "test_vector = np.reshape([float(test_counts[t]) for t in shared_vocab], (1, len(shared_vocab)))\n",
    "test_vector = normalize(test_vector)\n",
    "result = model.predict(test_vector)\n",
    "# display the prediction\n",
    "print result\n",
    "\n",
    "# display the word counts for our features\n",
    "print \"her:\", test_counts.get('her')\n",
    "print  \"the:\", test_counts.get('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
